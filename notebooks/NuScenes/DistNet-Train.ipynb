{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e282854-e652-4a3b-8546-8b9396762fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -U jupyterlab==3.0.16\n",
    "# !pip install ipywidgets # --user\n",
    "# !pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37bc1a9-3bf8-4d6d-904a-0f47a15a578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07210a8-af1d-467a-8b7c-a524b5c3d285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from distnet import DistResNeXt50\n",
    "from dataset import NuScenes\n",
    "import matplotlib.patches as patches\n",
    "from albumentations import OneOf, Compose\n",
    "from albumentations import (HorizontalFlip,\n",
    "                            HueSaturationValue,\n",
    "                            RandomBrightnessContrast,\n",
    "                            Blur,\n",
    "                            GaussNoise,\n",
    "                            CLAHE,\n",
    "                            CoarseDropout,\n",
    "                            RGBShift,\n",
    "                            BboxParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311b2137-be59-4d7a-ac0f-81f60cee04e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set torch random seed\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcd31ed-ba13-4f21-a831-63ac6b83d965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483d89c0-7dfa-4dd5-8e68-4e23a3197968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an albumentations augmentation routine\n",
    "alb_aug_list = [HorizontalFlip(p=1),  \n",
    "                HueSaturationValue(hue_shift_limit=20,\n",
    "                                   sat_shift_limit=30,\n",
    "                                   val_shift_limit=20,\n",
    "                                   p=1),\n",
    "                RandomBrightnessContrast(brightness_limit=0.2,\n",
    "                                         contrast_limit=0.2,\n",
    "                                         brightness_by_max=True,\n",
    "                                         p=1), \n",
    "                Blur(blur_limit=7, #blurring kernel size\n",
    "                     p=1),\n",
    "                GaussNoise(var_limit=(10.0, 50.0), #Variance range for noise\n",
    "                           mean=0,\n",
    "                           p=1),\n",
    "                CLAHE(clip_limit=4.0, #Contrast limited adaptive histogram equalization\n",
    "                      tile_grid_size=(8, 8),\n",
    "                      p=1),\n",
    "                RGBShift(p=1)\n",
    "                ]\n",
    "\n",
    "train_aug = OneOf(alb_aug_list, p=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bb5b59-41ac-418b-ab52-70b2256d69f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a transforms routine\n",
    "transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                             torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                              std=[0.229, 0.224, 0.225])\n",
    "                                            ])\n",
    "\n",
    "# create training dataset and loader\n",
    "size = 1024\n",
    "train_data = NuScenes(img_dir='/irad_mounts/lambda-quad-5-data/beholder/nuscenes/',\n",
    "                      meta_path='/irad_mounts/lambda-quad-5-data/beholder/nuscenes/nuscenes-v1.0-mini.csv',\n",
    "                      augs = train_aug,\n",
    "                      transforms=transforms,\n",
    "                      size=size)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=4, drop_last=True, shuffle=True, collate_fn=train_data.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aea8dd8-aca1-4f9b-b101-ad50107148b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot some example training data\n",
    "idx = 14\n",
    "image,boxes,distances,classes = train_data.__getitem__(idx)\n",
    "image,boxes,classes = image.numpy(),boxes.numpy(),classes.numpy()\n",
    "fig,ax = plt.subplots(figsize=(20,16))\n",
    "ax.imshow(np.transpose(image, (1,2,0)))\n",
    "for i in range(len(boxes)):\n",
    "    rect = patches.Rectangle((boxes[i][0],boxes[i][3]),boxes[i][2]-boxes[i][0],boxes[i][1]-boxes[i][3], linewidth=2, edgecolor='r', facecolor='none', label='object')\n",
    "    ax.add_patch(rect)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f0e1d3-efa2-43fd-ac7b-ece264b9e70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test dataset and loader\n",
    "test_data = NuScenes(img_dir='/irad_mounts/lambda-quad-5-data/beholder/nuscenes/',\n",
    "                     meta_path='/irad_mounts/lambda-quad-5-data/beholder/nuscenes/nuscenes-v1.0-mini.csv',\n",
    "                     transforms=transforms, size=size, split='test')\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=4, drop_last=True, shuffle=False, collate_fn=test_data.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066db719-4e61-4be8-9b8a-a372911b3ad0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot some example test data\n",
    "idx = 2\n",
    "image,boxes,distances,classes = test_data.__getitem__(idx)\n",
    "image,boxes,classes = image.numpy(),boxes.numpy(),classes.numpy()\n",
    "fig,ax = plt.subplots(figsize=(20,16))\n",
    "ax.imshow(np.transpose(image, (1,2,0)))\n",
    "for i in range(len(boxes)):\n",
    "    rect = patches.Rectangle((boxes[i][0],boxes[i][3]),boxes[i][2]-boxes[i][0],boxes[i][1]-boxes[i][3], linewidth=2, edgecolor='r', facecolor='none', label='object')\n",
    "    ax.add_patch(rect)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68d7ae0-bbc1-4a5c-bbf9-ca6af8895b00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load in our model\n",
    "model = DistResNeXt50(n_classes=train_data.n_classes, image_size=size)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedc0620-769e-49d2-8704-f1e824f55715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our loss function\n",
    "class_criterion = torch.nn.CrossEntropyLoss()\n",
    "dist_criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6634ff-4e2b-4156-a7ff-1cf7713e675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some hyperparameters\n",
    "n_epochs = 60\n",
    "lr = 1e-7\n",
    "weight_decay=1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e23348-6989-4fe7-a18c-081d4bc3ff4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define our training routine\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # train loop\n",
    "    running_train_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    print('Training...')\n",
    "    for i,data in enumerate(trainloader, 0):\n",
    "\n",
    "        # grab the batch and move the batch to the gpu\n",
    "        inputs,boxes,distances,classes = data[0],data[1],data[2],data[3]\n",
    "        inputs = inputs.to(device)\n",
    "        boxes = [b.to(device) for b in boxes]\n",
    "        distances = torch.cat([d.to(device) for d in distances])\n",
    "        classes = torch.cat([c.to(device) for c in classes])\n",
    "\n",
    "        # zero param gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward, backward, optimize\n",
    "        class_preds,distance_preds = model(inputs, boxes)\n",
    "        loss = class_criterion(class_preds.squeeze(), classes) + 10.0*dist_criterion(distance_preds.squeeze(), distances)\n",
    "        running_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i%100)==0:\n",
    "            print('batch [{0:}] - loss: {1:.3f}'.format(i,running_train_loss/(i+1)))\n",
    "\n",
    "    # end of epoch train stats\n",
    "    train_loss = running_train_loss/(i+1)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # validation loop\n",
    "    running_val_loss = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    print('Performing eval...')\n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate(testloader, 0):\n",
    "            \n",
    "            # grab the batch and move the batch to the gpu\n",
    "            inputs,boxes,distances,classes = data[0],data[1],data[2],data[3]\n",
    "            inputs = inputs.to(device)\n",
    "            boxes = [b.to(device) for b in boxes]\n",
    "            distances = torch.cat([d.to(device) for d in distances])\n",
    "            classes = torch.cat([c.to(device) for c in classes])\n",
    "\n",
    "            # forward, backward, optimize\n",
    "            class_preds,distance_preds = model(inputs, boxes)\n",
    "            loss = class_criterion(class_preds.squeeze(), classes) + 10.0*dist_criterion(distance_preds.squeeze(), distances)\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "    # calculate final val loss and step scheduler\n",
    "    val_loss = running_val_loss/(i+1)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # save the model every epoch\n",
    "    torch.save(model.state_dict(), '/irad_mounts/lambda-quad-5-data/beholder/distnet_weights/distnet_resnext_epoch_{}.pth'.format(epoch))\n",
    "    \n",
    "    # print stats\n",
    "    print('End of epoch stats...')\n",
    "    print('--------------------------------------------------------------------------------------')\n",
    "    print('epoch [{0:}] - train_loss: {1:.3f} - val_loss: {2:.3f}'.format(epoch, train_loss, val_loss))\n",
    "    print('--------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad1a5dd-8518-467d-bd17-5fb0e57a3ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "fig,ax = plt.subplots(1, sharex=True, figsize=(6,6))\n",
    "ax.plot(train_losses)\n",
    "ax.plot(val_losses)\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend(['train', 'val'])\n",
    "plt.savefig('losses.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df8b9f4-b1e2-4ab4-a9f2-b7eda665475b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
